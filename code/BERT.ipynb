{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZEZ5zF_3-8ZF",
        "ZXKOzVzf_-D-",
        "sctVm3PqUpCH"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqDx9HqVpGsW"
      },
      "source": [
        "#**Implementation of BERT on IMDB dataset**\n",
        "The following notebook illustrates the implemetation of BERT on the IMDB dataset\n",
        "Upload the IMDB dataset.csv file to run the notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM3X8iVcp3r-"
      },
      "source": [
        "#**Initializing variables and Setting up the Environemnt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlyqVowT6C8f"
      },
      "source": [
        "# Set environment seed\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZlRQ80c6wtU",
        "outputId": "0310424b-36ff-43a1-f258-93ebc60a5992"
      },
      "source": [
        "!pip install pytorch-nlp\n",
        "!pip install pytorch-pretrained-bert\n",
        "#!pip install sklearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.18.5)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/31/4d4861a90d66c287a348fd17eaefefcdc2e859951cab9884b555923f046d/boto3-1.16.23-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/49/c8c99477416fdebb59078bda624acc5b3c7008f891c60d56d6ff1570d83e/botocore-1.19.23-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.23->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.23->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.23 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.23 botocore-1.19.23 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31OQbAN7SOl"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da4P6zLk9W4F"
      },
      "source": [
        "def reset_random_seeds():\n",
        "    '''\n",
        "    Sets all necessary seed for reproduceability.\n",
        "    '''\n",
        "    os.environ['PYTHONHASHSEED']=str(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.cuda.manual_seed(1)\n",
        "    np.random.seed(1)\n",
        "    \n",
        "reset_random_seeds()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOHhprqYAQyv"
      },
      "source": [
        "#**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "I2-gOgp7MX9I",
        "outputId": "4aa622ad-d8d6-4ef1-b941-04a5604f5de7"
      },
      "source": [
        "#Reading the csv dataset\n",
        "data = pd.read_csv('IMDB Dataset.csv')\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scLRpf6W9Nla"
      },
      "source": [
        "#spliting the data into test and train \n",
        "x_train, x_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], \n",
        "                                                    test_size=0.2, random_state=0, \n",
        "                                                    stratify=data['sentiment'])\n",
        "\n",
        "x_test = x_test.to_list()\n",
        "y_test = y_test.to_list()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGy0Z523BslV"
      },
      "source": [
        "# Create a new df using the train set.\n",
        "data = {\n",
        "    'review':x_train,\n",
        "    'sentiment':y_train\n",
        "}\n",
        "\n",
        "data_split = pd.DataFrame(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zug2Tk8DCJLK"
      },
      "source": [
        "# Create a dictionary of x_train and y_train with 12 different split sizes\n",
        "data_dict = {}\n",
        "split_size = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "for split in split_size:\n",
        "    _, x_train, _, y_train = train_test_split(data_split['review'], data_split['sentiment'], \n",
        "                                                    test_size=split, random_state=0, \n",
        "                                                    stratify=data_split['sentiment'])\n",
        "    df = {\n",
        "        'review':x_train,\n",
        "        'sentiment':y_train\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(df)\n",
        "    data_dict[split] = df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD9UgdbXCWsS"
      },
      "source": [
        "# Specifying the split size\n",
        "data_dict.keys()\n",
        "split = 0.7"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhWlYgjgHJdc",
        "outputId": "cf7d3090-fed1-4f48-c577-107f5bdcc9a7"
      },
      "source": [
        "#Obtaining the x_train and y_train for the respective split size \n",
        "x_train = data_dict[split]['review']\n",
        "y_train = data_dict[split]['sentiment']\n",
        "print(x_train)\n",
        "print(y_train)\n",
        "y_train = y_train.to_list()\n",
        "x_train = x_train.to_list()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34919    This game was really great and quite a challen...\n",
            "32093    Superb comic farce from Paul Mazursky, Richard...\n",
            "23874    A brash, self-centered Army cadet arrives at W...\n",
            "7297     This one and \"Her Pilgrim Soul\" are two of my ...\n",
            "47940    David Tennant and Sarah Parish's brilliant act...\n",
            "                               ...                        \n",
            "21892    demonicus rocked, you guys need to understand ...\n",
            "28814    Okay, now I am pretty sure that my summary got...\n",
            "44744    \"Laughter is a state of mind\" says the tag, an...\n",
            "9248     I had initially heard of TEARS OF KALI a while...\n",
            "45490    I finally snagged a copy of Kannathil Muthamit...\n",
            "Name: review, Length: 28000, dtype: object\n",
            "34919    positive\n",
            "32093    positive\n",
            "23874    positive\n",
            "7297     positive\n",
            "47940    positive\n",
            "           ...   \n",
            "21892    positive\n",
            "28814    negative\n",
            "44744    negative\n",
            "9248     positive\n",
            "45490    positive\n",
            "Name: sentiment, Length: 28000, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA2OR-4_BvZh",
        "outputId": "72f4e612-e45e-4ab8-9367-382733492a3b"
      },
      "source": [
        "#Printing the number of entries in train and test\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This game was really great and quite a challenge. It has a great, spooky story line and the graphics are also very good. I would recommend this game to all Horror fans and is very gripping from start to finish. The only problem with this game is that i would have liked more weapons but thats just me.<br /><br /> A truly great game for RPG and Shoot'em'up fans.<br /><br />>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28000, 28000, 10000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXKOzVzf_-D-"
      },
      "source": [
        "#**Tokenising**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxfQHMqz_zwW",
        "outputId": "b43061ea-f8ca-42f8-d3d6-4e6766dba314"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 19362211.97B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaeB2jjQAW2y",
        "outputId": "ba177faa-4809-4919-d630-5d29bff213c0"
      },
      "source": [
        "tokenizer.tokenize('Hi my name is Atul')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'my', 'name', 'is', 'at', '##ul']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcoB01ixAfgb",
        "outputId": "07b0537e-de4a-4da8-f217-207c55ae1daf"
      },
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], x_train))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], x_test))\n",
        "\n",
        "len(train_tokens), len(test_tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sctVm3PqUpCH"
      },
      "source": [
        "#**Vectorising and masking inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjWAkOV9T1kc",
        "outputId": "f455d2b0-e060-40f2-81be-103f5a59eeff"
      },
      "source": [
        "#Reshaping the input size as (no of records,512)\n",
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28000, 512), (10000, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iq6hCgvUmSr"
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNBxClpQVFaf",
        "outputId": "7dceb2ec-88b0-4c86-d5ff-20551fc84a9c"
      },
      "source": [
        "# converting the labels from positive and negative to true and false\n",
        "train_y = np.array(y_train) == 'positive'\n",
        "test_y = np.array(y_test) == 'positive'\n",
        "print(train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y))\n",
        "train_y[1:20]\n",
        "test_y[1:20]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28000,) (10000,) 0.5 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "        True,  True, False, False,  True, False, False, False, False,\n",
              "       False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92qeYaLTcHUQ",
        "outputId": "fde05ad9-71e8-48cc-962a-9faf8a22c9dd"
      },
      "source": [
        "#Double checking that the number of False and True is 50% for both train and test labesls\n",
        "from collections import Counter\n",
        "Counter(train_y)\n",
        "Counter(test_y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({False: 5000, True: 5000})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZMXO2BOVNnL"
      },
      "source": [
        "#Initialising masks for train and test dataset \n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuCwqSVqdMZK"
      },
      "source": [
        "#**BERT Model**\n",
        "Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is – BERT is based on the Transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptmxLC5dTEp"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EocoT_e5dXOc",
        "outputId": "353e30d2-14d1-483a-de16-f53265eef9ec"
      },
      "source": [
        "# ensuring that the model runs on GPU, not on CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TAFY7qMbda9Y",
        "outputId": "497f84b2-62fc-4d60-e576-9188eeb07172"
      },
      "source": [
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.0M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJbcx6rfdeQH",
        "outputId": "31faf192-2c28-44f5-dc33-e8fb73315fe4"
      },
      "source": [
        "#Initialising bert model\n",
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:32<00:00, 12518281.19B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LL8yQ7SYdjME",
        "outputId": "13578003-e3e1-4446-f91a-a45ff2f0b98c"
      },
      "source": [
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'439.065088M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnugIPZUdn4i",
        "outputId": "1611beb2-264e-4045-c00b-b81c0c55e8c9"
      },
      "source": [
        "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
        "y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n",
        "x.shape, y.shape, pooled.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iElotzFTdsu4",
        "outputId": "88f00982-6658-4838-ddc0-1dbba21ad77f"
      },
      "source": [
        "y = bert_clf(x)\n",
        "y.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5572745],\n",
              "       [0.663245 ],\n",
              "       [0.5770542]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NSOc7JFVdu6x",
        "outputId": "4f9f2b5c-e955-47ad-d1c9-0a1d090cf803"
      },
      "source": [
        "# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'6697.349632M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WaetF6TUd5Ja",
        "outputId": "e9bf347a-d373-480e-96dd-ff617f4520e0"
      },
      "source": [
        "y, x, pooled = None, None, None\n",
        "torch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'439.065088M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex1cvWaDd_y-"
      },
      "source": [
        "# Setting hyper-parameters\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a3f9BMoPeDWP",
        "outputId": "18482173-f39a-4362-86ce-8cd7d876af16"
      },
      "source": [
        "# Conwverting inputs to tensor and applying mask on the train and test tensors\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'439.065088M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwarP8DueF-j"
      },
      "source": [
        "#Initaiseing dataloader for train and test tensors\n",
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9BJxTN2eKBS"
      },
      "source": [
        "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjbWW_C8eMOW"
      },
      "source": [
        "#Defining Optimizer\n",
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xg4fmklePGB"
      },
      "source": [
        "torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ztIbrHYeST0",
        "outputId": "b4e607af-c8d3-4d9e-bca0-5b79265b8498"
      },
      "source": [
        "#Trainining model\n",
        "val_acc = []\n",
        "val_loss = []\n",
        "train_losses = []\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        \n",
        "        loss_func = nn.BCELoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        \n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        \n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(x_train) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "    train_losses.append(train_loss / (step_num + 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "\r973/3500.0 loss: 0.36442436724977695 \n",
            "1816.352256M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7fOSEKH-4TO"
      },
      "source": [
        "# Save recorded train loss into a CSV file\n",
        "pd.DataFrame(np.array(train_losses),\n",
        "                   columns=['Loss']).to_csv('./train_loss.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtzoA9MimlE"
      },
      "source": [
        "# Evaluate Bert model with the test set\n",
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "loss = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss.append(loss_func(logits, labels))\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "print(sum(loss)/len(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmHL_78Qmtoo"
      },
      "source": [
        "# >0.5 means that the model is slightly bias towards positive sentiment\n",
        "# <0.5 means that the model is slightly bias towards negative sentiment\n",
        "np.mean(bert_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wndkuydsm1Av"
      },
      "source": [
        "# Get classificaqtion report and save it in the csv file\n",
        "print(classification_report(test_y, bert_predicted))\n",
        "report = classification_report(test_y, bert_predicted,output_dict=True)\n",
        "df = pd.DataFrame(data=report).transpose()\n",
        "df.to_csv('./report.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}