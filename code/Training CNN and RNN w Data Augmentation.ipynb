{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from random import sample\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RNN hyper parameters\n",
    "\n",
    "rnn_hyper_param = {\n",
    "    'VOCAB_SIZE':50000,\n",
    "    'EPOCHS':150,\n",
    "    'BS':1024,\n",
    "    'LR':0.005,\n",
    "    'OOV_TOK':\"<OOV>\",\n",
    "    'MAX_LENGTH':520,\n",
    "    'PADDING_TYPE':\"post\",\n",
    "    'TRUNC_TYPE':\"post\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CNN hyper parameters\n",
    "\n",
    "cnn_hyper_param = {\n",
    "    'VOCAB_SIZE':50000,\n",
    "    'EPOCHS':150,\n",
    "    'BS':1024,\n",
    "    'LR':0.001,\n",
    "    'OOV_TOK':\"<OOV>\",\n",
    "    'MAX_LENGTH':520,\n",
    "    'PADDING_TYPE':\"post\",\n",
    "    'TRUNC_TYPE':\"post\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep to the same size of test set we've been using to test how well it generalizes.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], \n",
    "                                                    test_size=0.2, random_state=0, \n",
    "                                                    stratify=data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tokenization and apply the dictioanry on our train and test set.\n",
    "# Convert all categorcial data to numpy array of integers\n",
    "\n",
    "tokenizer = Tokenizer(num_words = rnn_hyper_param['VOCAB_SIZE'], oov_token=rnn_hyper_param['OOV_TOK'])\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "testing_padded = pad_sequences(test_sequences, padding=rnn_hyper_param['PADDING_TYPE'], \n",
    "                               maxlen=rnn_hyper_param['MAX_LENGTH'], truncating=rnn_hyper_param['TRUNC_TYPE'])\n",
    "y_test = list(y_test)\n",
    "for i in range (len(y_test)):\n",
    "    if y_test[i] == 'positive':\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgNestedLists(nested_vals):\n",
    "    \"\"\"\n",
    "    Averages a 2-D array and returns a 1-D array of all of the columns\n",
    "    averaged together, regardless of their dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = []\n",
    "    maximum = 0\n",
    "    for lst in nested_vals:\n",
    "        if len(lst) > maximum:\n",
    "            maximum = len(lst)\n",
    "    for index in range(maximum): # Go through each index of longest list\n",
    "        temp = []\n",
    "        for lst in nested_vals: # Go through each list\n",
    "            if index < len(lst): # If not an index error\n",
    "                temp.append(lst[index])\n",
    "        output.append(np.nanmean(temp))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Create Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates a CNN model, compiles and returns it\n",
    "\n",
    "def create_cnn():\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Embedding(len(word_index)+1, 30, input_length=520),\n",
    "    \n",
    "    keras.layers.Conv1D(40, 3, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv1D(5, 3, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(20, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=cnn_hyper_param['LR']), \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function creates an RNN model, compiles and returns it\n",
    "\n",
    "def create_rnn():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(len(word_index)+1, 128, input_length=520),\n",
    "        keras.layers.LSTM(100, return_sequences=True),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.LSTM(100),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=rnn_hyper_param['LR']),\n",
    "                     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Best CNN with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the callbacks pateince values as decided after analyzing the Vanilla CNN\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=4, min_lr=0.00005, verbose=1)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using 0.01 of dataset.\n",
      "Train on 6800 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "6800/6800 [==============================] - 4s 523us/sample - loss: 0.6919 - accuracy: 0.5191 - val_loss: 0.6931 - val_accuracy: 0.5025\n",
      "Epoch 2/250\n",
      "6800/6800 [==============================] - 1s 152us/sample - loss: 0.6839 - accuracy: 0.5999 - val_loss: 0.6934 - val_accuracy: 0.5065\n",
      "Epoch 3/250\n",
      "6800/6800 [==============================] - 1s 154us/sample - loss: 0.6669 - accuracy: 0.6269 - val_loss: 0.6948 - val_accuracy: 0.5027\n",
      "Epoch 4/250\n",
      "6800/6800 [==============================] - 1s 155us/sample - loss: 0.6375 - accuracy: 0.6906 - val_loss: 0.6948 - val_accuracy: 0.5061\n",
      "Epoch 5/250\n",
      "6144/6800 [==========================>...] - ETA: 0s - loss: 0.5908 - accuracy: 0.7705\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "6800/6800 [==============================] - 1s 162us/sample - loss: 0.5870 - accuracy: 0.7754 - val_loss: 0.6985 - val_accuracy: 0.5080\n",
      "Epoch 6/250\n",
      "6800/6800 [==============================] - 1s 156us/sample - loss: 0.5362 - accuracy: 0.8143 - val_loss: 0.6988 - val_accuracy: 0.5081\n",
      "Epoch 7/250\n",
      "6800/6800 [==============================] - 1s 159us/sample - loss: 0.5158 - accuracy: 0.8325 - val_loss: 0.6995 - val_accuracy: 0.5104\n",
      "Epoch 8/250\n",
      "6800/6800 [==============================] - 1s 154us/sample - loss: 0.4930 - accuracy: 0.8550 - val_loss: 0.7015 - val_accuracy: 0.5102\n",
      "Epoch 9/250\n",
      "6144/6800 [==========================>...] - ETA: 0s - loss: 0.4726 - accuracy: 0.8691\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "6800/6800 [==============================] - 1s 153us/sample - loss: 0.4726 - accuracy: 0.8687 - val_loss: 0.7030 - val_accuracy: 0.5088\n",
      "Epoch 10/250\n",
      "6800/6800 [==============================] - 1s 154us/sample - loss: 0.4530 - accuracy: 0.8838 - val_loss: 0.7030 - val_accuracy: 0.5094\n",
      "Epoch 11/250\n",
      "6800/6800 [==============================] - 1s 157us/sample - loss: 0.4464 - accuracy: 0.8797 - val_loss: 0.7034 - val_accuracy: 0.5089\n",
      "Epoch 12/250\n",
      "6800/6800 [==============================] - 1s 155us/sample - loss: 0.4394 - accuracy: 0.8874 - val_loss: 0.7038 - val_accuracy: 0.5091\n",
      "Epoch 13/250\n",
      "6800/6800 [==============================] - 1s 153us/sample - loss: 0.4356 - accuracy: 0.8929 - val_loss: 0.7042 - val_accuracy: 0.5097\n",
      "Epoch 00013: early stopping\n",
      "Training using 0.05 of dataset.\n",
      "Train on 18000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "18000/18000 [==============================] - 4s 213us/sample - loss: 0.6928 - accuracy: 0.5142 - val_loss: 0.6930 - val_accuracy: 0.5135\n",
      "Epoch 2/250\n",
      "18000/18000 [==============================] - 2s 132us/sample - loss: 0.6861 - accuracy: 0.5702 - val_loss: 0.6919 - val_accuracy: 0.5251\n",
      "Epoch 3/250\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.6571 - accuracy: 0.6562 - val_loss: 0.6854 - val_accuracy: 0.5593\n",
      "Epoch 4/250\n",
      "18000/18000 [==============================] - 2s 135us/sample - loss: 0.5355 - accuracy: 0.8054 - val_loss: 0.6269 - val_accuracy: 0.6728\n",
      "Epoch 5/250\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.2705 - accuracy: 0.9291 - val_loss: 0.5117 - val_accuracy: 0.7526\n",
      "Epoch 6/250\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.0889 - accuracy: 0.9833 - val_loss: 0.5257 - val_accuracy: 0.7707\n",
      "Epoch 7/250\n",
      "18000/18000 [==============================] - 2s 135us/sample - loss: 0.0370 - accuracy: 0.9935 - val_loss: 0.5632 - val_accuracy: 0.7838\n",
      "Epoch 8/250\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.0207 - accuracy: 0.9964 - val_loss: 0.5726 - val_accuracy: 0.7939\n",
      "Epoch 9/250\n",
      "17408/18000 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9978\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18000/18000 [==============================] - 2s 133us/sample - loss: 0.0136 - accuracy: 0.9977 - val_loss: 0.6304 - val_accuracy: 0.7943\n",
      "Epoch 10/250\n",
      "18000/18000 [==============================] - 2s 137us/sample - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.6313 - val_accuracy: 0.7960\n",
      "Epoch 11/250\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.0107 - accuracy: 0.9981 - val_loss: 0.6518 - val_accuracy: 0.7939\n",
      "Epoch 12/250\n",
      "18000/18000 [==============================] - 2s 129us/sample - loss: 0.0105 - accuracy: 0.9982 - val_loss: 0.6336 - val_accuracy: 0.7983\n",
      "Epoch 13/250\n",
      "17408/18000 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9983\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "18000/18000 [==============================] - 2s 134us/sample - loss: 0.0095 - accuracy: 0.9984 - val_loss: 0.6403 - val_accuracy: 0.7997\n",
      "Epoch 14/250\n",
      "18000/18000 [==============================] - 2s 133us/sample - loss: 0.0094 - accuracy: 0.9983 - val_loss: 0.6408 - val_accuracy: 0.7990\n",
      "Epoch 15/250\n",
      "18000/18000 [==============================] - 2s 136us/sample - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.6445 - val_accuracy: 0.7990\n",
      "Epoch 16/250\n",
      "18000/18000 [==============================] - 2s 135us/sample - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.6448 - val_accuracy: 0.7993\n",
      "Epoch 17/250\n",
      "18000/18000 [==============================] - 3s 146us/sample - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.6464 - val_accuracy: 0.7987\n",
      "Epoch 00017: early stopping\n",
      "Training using 0.1 of dataset.\n",
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "20000/20000 [==============================] - 4s 180us/sample - loss: 0.6928 - accuracy: 0.5148 - val_loss: 0.6930 - val_accuracy: 0.5081\n",
      "Epoch 2/250\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.6901 - accuracy: 0.5329 - val_loss: 0.6925 - val_accuracy: 0.5252\n",
      "Epoch 3/250\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.6815 - accuracy: 0.5702 - val_loss: 0.6854 - val_accuracy: 0.5526\n",
      "Epoch 4/250\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.6175 - accuracy: 0.6862 - val_loss: 0.5752 - val_accuracy: 0.7122\n",
      "Epoch 5/250\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.3467 - accuracy: 0.8686 - val_loss: 0.4139 - val_accuracy: 0.8194\n",
      "Epoch 6/250\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.1240 - accuracy: 0.9610 - val_loss: 0.4428 - val_accuracy: 0.8327\n",
      "Epoch 7/250\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.0507 - accuracy: 0.9858 - val_loss: 0.5316 - val_accuracy: 0.8348\n",
      "Epoch 8/250\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.0256 - accuracy: 0.9938 - val_loss: 0.6051 - val_accuracy: 0.8341\n",
      "Epoch 9/250\n",
      "19456/20000 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9966\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.0156 - accuracy: 0.9966 - val_loss: 0.6699 - val_accuracy: 0.8318\n",
      "Epoch 10/250\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.0141 - accuracy: 0.9965 - val_loss: 0.6797 - val_accuracy: 0.8323\n",
      "Epoch 11/250\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.0117 - accuracy: 0.9976 - val_loss: 0.6890 - val_accuracy: 0.8333\n",
      "Epoch 12/250\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.0110 - accuracy: 0.9973 - val_loss: 0.6986 - val_accuracy: 0.8340\n",
      "Epoch 13/250\n",
      "19456/20000 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9972\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.0103 - accuracy: 0.9973 - val_loss: 0.7083 - val_accuracy: 0.8312\n",
      "Epoch 14/250\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.7087 - val_accuracy: 0.8342\n",
      "Epoch 15/250\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.7109 - val_accuracy: 0.8339\n",
      "Epoch 16/250\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.7134 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/250\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.7162 - val_accuracy: 0.8334\n",
      "Epoch 00017: early stopping\n",
      "Training using 0.2 of dataset.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "40000/40000 [==============================] - 6s 157us/sample - loss: 0.6924 - accuracy: 0.5160 - val_loss: 0.6918 - val_accuracy: 0.5246\n",
      "Epoch 2/250\n",
      "40000/40000 [==============================] - 5s 120us/sample - loss: 0.6755 - accuracy: 0.5811 - val_loss: 0.6281 - val_accuracy: 0.6686\n",
      "Epoch 3/250\n",
      "40000/40000 [==============================] - 5s 120us/sample - loss: 0.4151 - accuracy: 0.8188 - val_loss: 0.3334 - val_accuracy: 0.8609\n",
      "Epoch 4/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.1290 - accuracy: 0.9577 - val_loss: 0.3955 - val_accuracy: 0.8657\n",
      "Epoch 5/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.0530 - accuracy: 0.9837 - val_loss: 0.4941 - val_accuracy: 0.8603\n",
      "Epoch 6/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.0275 - accuracy: 0.9926 - val_loss: 0.5754 - val_accuracy: 0.8596\n",
      "Epoch 7/250\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9949\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "40000/40000 [==============================] - 5s 122us/sample - loss: 0.0194 - accuracy: 0.9949 - val_loss: 0.6397 - val_accuracy: 0.8599\n",
      "Epoch 8/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.6460 - val_accuracy: 0.8604\n",
      "Epoch 9/250\n",
      "40000/40000 [==============================] - 5s 120us/sample - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.6593 - val_accuracy: 0.8614\n",
      "Epoch 10/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.6800 - val_accuracy: 0.8618\n",
      "Epoch 11/250\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9976\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "40000/40000 [==============================] - 5s 131us/sample - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.6918 - val_accuracy: 0.8615\n",
      "Epoch 12/250\n",
      "40000/40000 [==============================] - 5s 122us/sample - loss: 0.0104 - accuracy: 0.9974 - val_loss: 0.6957 - val_accuracy: 0.8607\n",
      "Epoch 13/250\n",
      "40000/40000 [==============================] - 5s 128us/sample - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.6989 - val_accuracy: 0.8608\n",
      "Epoch 14/250\n",
      "40000/40000 [==============================] - 5s 122us/sample - loss: 0.0096 - accuracy: 0.9980 - val_loss: 0.7042 - val_accuracy: 0.8610\n",
      "Epoch 15/250\n",
      "40000/40000 [==============================] - 5s 121us/sample - loss: 0.0094 - accuracy: 0.9980 - val_loss: 0.7080 - val_accuracy: 0.8606\n",
      "Epoch 00015: early stopping\n",
      "Training using 0.3 of dataset.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.6927 - accuracy: 0.5083 - val_loss: 0.6897 - val_accuracy: 0.5351\n",
      "Epoch 2/250\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5355 - accuracy: 0.7185 - val_loss: 0.3228 - val_accuracy: 0.8688\n",
      "Epoch 3/250\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.1571 - accuracy: 0.9468 - val_loss: 0.3591 - val_accuracy: 0.8774\n",
      "Epoch 4/250\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0564 - accuracy: 0.9832 - val_loss: 0.4969 - val_accuracy: 0.8696\n",
      "Epoch 5/250\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0353 - accuracy: 0.9900 - val_loss: 0.5689 - val_accuracy: 0.8710\n",
      "Epoch 6/250\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9934\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0238 - accuracy: 0.9935 - val_loss: 0.6567 - val_accuracy: 0.8671\n",
      "Epoch 7/250\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0159 - accuracy: 0.9961 - val_loss: 0.6730 - val_accuracy: 0.8681\n",
      "Epoch 8/250\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0143 - accuracy: 0.9967 - val_loss: 0.6887 - val_accuracy: 0.8690\n",
      "Epoch 9/250\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.7254 - val_accuracy: 0.8694\n",
      "Epoch 10/250\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.7311 - val_accuracy: 0.8704\n",
      "Epoch 11/250\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0100 - accuracy: 0.9978 - val_loss: 0.7376 - val_accuracy: 0.8708\n",
      "Epoch 12/250\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0103 - accuracy: 0.9976 - val_loss: 0.7438 - val_accuracy: 0.8700\n",
      "Epoch 13/250\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0095 - accuracy: 0.9978 - val_loss: 0.7483 - val_accuracy: 0.8707\n",
      "Epoch 14/250\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.7539 - val_accuracy: 0.8705\n",
      "Epoch 00014: early stopping\n",
      "Training using 0.4 of dataset.\n",
      "Train on 80000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "80000/80000 [==============================] - 11s 132us/sample - loss: 0.6920 - accuracy: 0.5163 - val_loss: 0.6820 - val_accuracy: 0.6573\n",
      "Epoch 2/250\n",
      "80000/80000 [==============================] - 9s 116us/sample - loss: 0.4057 - accuracy: 0.8153 - val_loss: 0.2934 - val_accuracy: 0.8850\n",
      "Epoch 3/250\n",
      "80000/80000 [==============================] - 9s 117us/sample - loss: 0.1148 - accuracy: 0.9629 - val_loss: 0.4737 - val_accuracy: 0.8695\n",
      "Epoch 4/250\n",
      "80000/80000 [==============================] - 9s 117us/sample - loss: 0.0591 - accuracy: 0.9819 - val_loss: 0.5054 - val_accuracy: 0.8786\n",
      "Epoch 5/250\n",
      "80000/80000 [==============================] - 9s 118us/sample - loss: 0.0360 - accuracy: 0.9899 - val_loss: 0.5698 - val_accuracy: 0.8757\n",
      "Epoch 6/250\n",
      "79872/80000 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9930\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "80000/80000 [==============================] - 10s 119us/sample - loss: 0.0260 - accuracy: 0.9930 - val_loss: 0.6370 - val_accuracy: 0.8737\n",
      "Epoch 7/250\n",
      "80000/80000 [==============================] - 10s 121us/sample - loss: 0.0179 - accuracy: 0.9956 - val_loss: 0.6674 - val_accuracy: 0.8749\n",
      "Epoch 8/250\n",
      "80000/80000 [==============================] - 9s 118us/sample - loss: 0.0169 - accuracy: 0.9959 - val_loss: 0.6900 - val_accuracy: 0.8746\n",
      "Epoch 9/250\n",
      "80000/80000 [==============================] - 9s 117us/sample - loss: 0.0146 - accuracy: 0.9967 - val_loss: 0.7183 - val_accuracy: 0.8750\n",
      "Epoch 10/250\n",
      "79872/80000 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9967\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "80000/80000 [==============================] - 9s 118us/sample - loss: 0.0138 - accuracy: 0.9967 - val_loss: 0.7228 - val_accuracy: 0.8734\n",
      "Epoch 11/250\n",
      "80000/80000 [==============================] - 10s 120us/sample - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.7256 - val_accuracy: 0.8742\n",
      "Epoch 12/250\n",
      "80000/80000 [==============================] - 10s 119us/sample - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.7345 - val_accuracy: 0.8741\n",
      "Epoch 13/250\n",
      "80000/80000 [==============================] - 9s 118us/sample - loss: 0.0120 - accuracy: 0.9974 - val_loss: 0.7413 - val_accuracy: 0.8744\n",
      "Epoch 14/250\n",
      "80000/80000 [==============================] - 9s 118us/sample - loss: 0.0114 - accuracy: 0.9978 - val_loss: 0.7463 - val_accuracy: 0.8745\n",
      "Epoch 00014: early stopping\n",
      "Training using 0.5 of dataset.\n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "100000/100000 [==============================] - 13s 130us/sample - loss: 0.6659 - accuracy: 0.5656 - val_loss: 0.4609 - val_accuracy: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/250\n",
      "100000/100000 [==============================] - 12s 119us/sample - loss: 0.2478 - accuracy: 0.9065 - val_loss: 0.3225 - val_accuracy: 0.8805\n",
      "Epoch 3/250\n",
      "100000/100000 [==============================] - 12s 120us/sample - loss: 0.0988 - accuracy: 0.9678 - val_loss: 0.4160 - val_accuracy: 0.8780\n",
      "Epoch 4/250\n",
      "100000/100000 [==============================] - 12s 116us/sample - loss: 0.0562 - accuracy: 0.9822 - val_loss: 0.5376 - val_accuracy: 0.8705\n",
      "Epoch 5/250\n",
      "100000/100000 [==============================] - 12s 116us/sample - loss: 0.0386 - accuracy: 0.9879 - val_loss: 0.6304 - val_accuracy: 0.8727\n",
      "Epoch 6/250\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9917\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "100000/100000 [==============================] - 12s 119us/sample - loss: 0.0277 - accuracy: 0.9917 - val_loss: 0.6831 - val_accuracy: 0.8719\n",
      "Epoch 7/250\n",
      "100000/100000 [==============================] - 12s 124us/sample - loss: 0.0193 - accuracy: 0.9947 - val_loss: 0.7007 - val_accuracy: 0.8730\n",
      "Epoch 8/250\n",
      "100000/100000 [==============================] - 11s 115us/sample - loss: 0.0160 - accuracy: 0.9958 - val_loss: 0.7300 - val_accuracy: 0.8736\n",
      "Epoch 9/250\n",
      "100000/100000 [==============================] - 12s 117us/sample - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.7643 - val_accuracy: 0.8710\n",
      "Epoch 10/250\n",
      " 99328/100000 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9968\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "100000/100000 [==============================] - 12s 120us/sample - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.7829 - val_accuracy: 0.8716\n",
      "Epoch 11/250\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 0.0124 - accuracy: 0.9967 - val_loss: 0.7896 - val_accuracy: 0.8717\n",
      "Epoch 12/250\n",
      "100000/100000 [==============================] - 12s 116us/sample - loss: 0.0114 - accuracy: 0.9972 - val_loss: 0.8011 - val_accuracy: 0.8719\n",
      "Epoch 13/250\n",
      "100000/100000 [==============================] - 12s 120us/sample - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.8039 - val_accuracy: 0.8706\n",
      "Epoch 14/250\n",
      "100000/100000 [==============================] - 12s 116us/sample - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.8085 - val_accuracy: 0.8716\n",
      "Epoch 00014: early stopping\n",
      "Training using 0.6 of dataset.\n",
      "Train on 120000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "120000/120000 [==============================] - 16s 131us/sample - loss: 0.6697 - accuracy: 0.5574 - val_loss: 0.4229 - val_accuracy: 0.8400\n",
      "Epoch 2/250\n",
      "120000/120000 [==============================] - 14s 117us/sample - loss: 0.2340 - accuracy: 0.9154 - val_loss: 0.3032 - val_accuracy: 0.8911\n",
      "Epoch 3/250\n",
      "120000/120000 [==============================] - 14s 117us/sample - loss: 0.0914 - accuracy: 0.9721 - val_loss: 0.4159 - val_accuracy: 0.8906\n",
      "Epoch 4/250\n",
      "120000/120000 [==============================] - 14s 116us/sample - loss: 0.0548 - accuracy: 0.9844 - val_loss: 0.5079 - val_accuracy: 0.8841\n",
      "Epoch 5/250\n",
      "120000/120000 [==============================] - 14s 115us/sample - loss: 0.0394 - accuracy: 0.9893 - val_loss: 0.5837 - val_accuracy: 0.8811\n",
      "Epoch 6/250\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.9918\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "120000/120000 [==============================] - 14s 119us/sample - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.6677 - val_accuracy: 0.8781\n",
      "Epoch 7/250\n",
      "120000/120000 [==============================] - 14s 118us/sample - loss: 0.0229 - accuracy: 0.9948 - val_loss: 0.7069 - val_accuracy: 0.8782\n",
      "Epoch 8/250\n",
      "120000/120000 [==============================] - 14s 117us/sample - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.7338 - val_accuracy: 0.8793\n",
      "Epoch 9/250\n",
      "120000/120000 [==============================] - 17s 142us/sample - loss: 0.0172 - accuracy: 0.9966 - val_loss: 0.7689 - val_accuracy: 0.8807\n",
      "Epoch 10/250\n",
      "119808/120000 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9967\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "120000/120000 [==============================] - 15s 121us/sample - loss: 0.0157 - accuracy: 0.9967 - val_loss: 0.8056 - val_accuracy: 0.8799\n",
      "Epoch 11/250\n",
      "120000/120000 [==============================] - 14s 116us/sample - loss: 0.0146 - accuracy: 0.9972 - val_loss: 0.8082 - val_accuracy: 0.8790\n",
      "Epoch 12/250\n",
      "120000/120000 [==============================] - 14s 121us/sample - loss: 0.0131 - accuracy: 0.9975 - val_loss: 0.8154 - val_accuracy: 0.8785\n",
      "Epoch 13/250\n",
      "120000/120000 [==============================] - 14s 119us/sample - loss: 0.0142 - accuracy: 0.9973 - val_loss: 0.8245 - val_accuracy: 0.8792\n",
      "Epoch 14/250\n",
      "120000/120000 [==============================] - 14s 117us/sample - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.8347 - val_accuracy: 0.8788\n",
      "Epoch 00014: early stopping\n",
      "Training using 0.7 of dataset.\n",
      "Train on 140000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "140000/140000 [==============================] - 18s 129us/sample - loss: 0.5972 - accuracy: 0.6274 - val_loss: 0.2897 - val_accuracy: 0.8820\n",
      "Epoch 2/250\n",
      "140000/140000 [==============================] - 17s 121us/sample - loss: 0.1715 - accuracy: 0.9380 - val_loss: 0.3299 - val_accuracy: 0.8904\n",
      "Epoch 3/250\n",
      "140000/140000 [==============================] - 16s 117us/sample - loss: 0.0824 - accuracy: 0.9721 - val_loss: 0.4238 - val_accuracy: 0.8844\n",
      "Epoch 4/250\n",
      "140000/140000 [==============================] - 17s 119us/sample - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.5281 - val_accuracy: 0.8829\n",
      "Epoch 5/250\n",
      "139264/140000 [============================>.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9890\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "140000/140000 [==============================] - 17s 122us/sample - loss: 0.0360 - accuracy: 0.9890 - val_loss: 0.6031 - val_accuracy: 0.8810\n",
      "Epoch 6/250\n",
      "140000/140000 [==============================] - 16s 117us/sample - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.6416 - val_accuracy: 0.8825\n",
      "Epoch 7/250\n",
      "140000/140000 [==============================] - 18s 130us/sample - loss: 0.0204 - accuracy: 0.9942 - val_loss: 0.6816 - val_accuracy: 0.8815\n",
      "Epoch 8/250\n",
      "140000/140000 [==============================] - 18s 126us/sample - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.7209 - val_accuracy: 0.8807\n",
      "Epoch 9/250\n",
      "139264/140000 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9950\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "140000/140000 [==============================] - 17s 125us/sample - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.7414 - val_accuracy: 0.8789\n",
      "Epoch 10/250\n",
      "140000/140000 [==============================] - 17s 122us/sample - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.7561 - val_accuracy: 0.8798\n",
      "Epoch 11/250\n",
      "140000/140000 [==============================] - 17s 120us/sample - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.7613 - val_accuracy: 0.8806\n",
      "Epoch 12/250\n",
      "140000/140000 [==============================] - 18s 127us/sample - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.7711 - val_accuracy: 0.8797\n",
      "Epoch 13/250\n",
      "140000/140000 [==============================] - 19s 135us/sample - loss: 0.0132 - accuracy: 0.9962 - val_loss: 0.7791 - val_accuracy: 0.8802\n",
      "Epoch 00013: early stopping\n",
      "Training using 0.8 of dataset.\n",
      "Train on 160000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "160000/160000 [==============================] - 20s 128us/sample - loss: 0.5576 - accuracy: 0.6643 - val_loss: 0.2709 - val_accuracy: 0.8969\n",
      "Epoch 2/250\n",
      "160000/160000 [==============================] - 20s 122us/sample - loss: 0.1777 - accuracy: 0.9380 - val_loss: 0.3108 - val_accuracy: 0.8958\n",
      "Epoch 3/250\n",
      "160000/160000 [==============================] - 20s 122us/sample - loss: 0.0950 - accuracy: 0.9691 - val_loss: 0.3860 - val_accuracy: 0.8889\n",
      "Epoch 4/250\n",
      "160000/160000 [==============================] - 21s 128us/sample - loss: 0.0627 - accuracy: 0.9806 - val_loss: 0.4897 - val_accuracy: 0.8883\n",
      "Epoch 5/250\n",
      "159744/160000 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9855\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "160000/160000 [==============================] - 20s 125us/sample - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.5594 - val_accuracy: 0.8865\n",
      "Epoch 6/250\n",
      "160000/160000 [==============================] - 20s 123us/sample - loss: 0.0335 - accuracy: 0.9904 - val_loss: 0.6169 - val_accuracy: 0.8856\n",
      "Epoch 7/250\n",
      "160000/160000 [==============================] - 20s 126us/sample - loss: 0.0294 - accuracy: 0.9915 - val_loss: 0.6434 - val_accuracy: 0.8856\n",
      "Epoch 8/250\n",
      "160000/160000 [==============================] - 19s 117us/sample - loss: 0.0256 - accuracy: 0.9928 - val_loss: 0.6755 - val_accuracy: 0.8871\n",
      "Epoch 9/250\n",
      "159744/160000 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9935\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "160000/160000 [==============================] - 19s 120us/sample - loss: 0.0236 - accuracy: 0.9935 - val_loss: 0.7009 - val_accuracy: 0.8847\n",
      "Epoch 10/250\n",
      "160000/160000 [==============================] - 19s 119us/sample - loss: 0.0210 - accuracy: 0.9943 - val_loss: 0.7129 - val_accuracy: 0.8845\n",
      "Epoch 11/250\n",
      "160000/160000 [==============================] - 22s 140us/sample - loss: 0.0204 - accuracy: 0.9945 - val_loss: 0.7273 - val_accuracy: 0.8845\n",
      "Epoch 12/250\n",
      "160000/160000 [==============================] - 20s 126us/sample - loss: 0.0203 - accuracy: 0.9946 - val_loss: 0.7340 - val_accuracy: 0.8847\n",
      "Epoch 13/250\n",
      "160000/160000 [==============================] - 23s 141us/sample - loss: 0.0198 - accuracy: 0.9948 - val_loss: 0.7400 - val_accuracy: 0.8841\n",
      "Epoch 00013: early stopping\n",
      "Training using 0.9 of dataset.\n",
      "Train on 180000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "180000/180000 [==============================] - 27s 152us/sample - loss: 0.5216 - accuracy: 0.6915 - val_loss: 0.2542 - val_accuracy: 0.9022\n",
      "Epoch 2/250\n",
      "180000/180000 [==============================] - 25s 140us/sample - loss: 0.1534 - accuracy: 0.9473 - val_loss: 0.3288 - val_accuracy: 0.8969\n",
      "Epoch 3/250\n",
      "180000/180000 [==============================] - 24s 133us/sample - loss: 0.0822 - accuracy: 0.9730 - val_loss: 0.4086 - val_accuracy: 0.8940\n",
      "Epoch 4/250\n",
      "180000/180000 [==============================] - 23s 129us/sample - loss: 0.0552 - accuracy: 0.9822 - val_loss: 0.5127 - val_accuracy: 0.8880\n",
      "Epoch 5/250\n",
      "179200/180000 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9872\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "180000/180000 [==============================] - 25s 138us/sample - loss: 0.0410 - accuracy: 0.9872 - val_loss: 0.5505 - val_accuracy: 0.8909\n",
      "Epoch 6/250\n",
      "180000/180000 [==============================] - 24s 135us/sample - loss: 0.0276 - accuracy: 0.9918 - val_loss: 0.6132 - val_accuracy: 0.8903\n",
      "Epoch 7/250\n",
      "180000/180000 [==============================] - 25s 136us/sample - loss: 0.0232 - accuracy: 0.9932 - val_loss: 0.6590 - val_accuracy: 0.8884\n",
      "Epoch 8/250\n",
      "180000/180000 [==============================] - 26s 147us/sample - loss: 0.0199 - accuracy: 0.9942 - val_loss: 0.7005 - val_accuracy: 0.8878\n",
      "Epoch 9/250\n",
      "179200/180000 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9950\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "180000/180000 [==============================] - 25s 140us/sample - loss: 0.0174 - accuracy: 0.9950 - val_loss: 0.7357 - val_accuracy: 0.8882\n",
      "Epoch 10/250\n",
      "180000/180000 [==============================] - 27s 149us/sample - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.7469 - val_accuracy: 0.8870\n",
      "Epoch 11/250\n",
      "180000/180000 [==============================] - 23s 127us/sample - loss: 0.0160 - accuracy: 0.9955 - val_loss: 0.7597 - val_accuracy: 0.8861\n",
      "Epoch 12/250\n",
      "180000/180000 [==============================] - 24s 136us/sample - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.7603 - val_accuracy: 0.8869\n",
      "Epoch 13/250\n",
      "180000/180000 [==============================] - 23s 127us/sample - loss: 0.0147 - accuracy: 0.9960 - val_loss: 0.7715 - val_accuracy: 0.8869\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN on various augmented datasets and write the history results to csv\n",
    "\n",
    "CNN_aug_hist = {}\n",
    "for size, dataset in data_dict.items():\n",
    "    print(f\"Training using {size} of dataset.\")\n",
    "    \n",
    "    # read csv for that augmented file\n",
    "    _dataset = pd.read_csv(f'data_aug_{size}.csv')\n",
    "    x_train = list(_dataset['review'])\n",
    "    y_train = list(_dataset['sentiment'])\n",
    "    for j in range (len(y_train)):\n",
    "        if y_train[j]=='positive':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = np.array(y_train)\n",
    "    train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    training_padded = pad_sequences(train_sequences, padding=cnn_hyper_param['PADDING_TYPE'], \n",
    "                                        maxlen=cnn_hyper_param['MAX_LENGTH'], truncating=cnn_hyper_param['TRUNC_TYPE'])\n",
    "\n",
    "    CNN = create_cnn()\n",
    "    history = CNN.fit(training_padded, y_train, batch_size = cnn_hyper_param['BS'],\n",
    "                            epochs=250, validation_data=(testing_padded, y_test),\n",
    "                           callbacks=[reduce_lr, earlystop])\n",
    "\n",
    "    CNN_aug_hist[f'{size}'] = history\n",
    "    \n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    # save to csv:\n",
    "    hist_csv_file = 'CNN_Aug_History_2_' + str(size) + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "ITERATION 0\n",
      "================================================\n",
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.5131 - accuracy: 0.6984 - val_loss: 0.2586 - val_accuracy: 0.8972\n",
      "Epoch 2/250\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.1607 - accuracy: 0.9449 - val_loss: 0.3099 - val_accuracy: 0.8998\n",
      "Epoch 3/250\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.0908 - accuracy: 0.9701 - val_loss: 0.3733 - val_accuracy: 0.8944\n",
      "Epoch 4/250\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.0611 - accuracy: 0.9809 - val_loss: 0.4603 - val_accuracy: 0.8966\n",
      "Epoch 5/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0444 - accuracy: 0.9862\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.0444 - accuracy: 0.9862 - val_loss: 0.5463 - val_accuracy: 0.8931\n",
      "Epoch 6/250\n",
      "200000/200000 [==============================] - 25s 126us/sample - loss: 0.0308 - accuracy: 0.9908 - val_loss: 0.5922 - val_accuracy: 0.8940\n",
      "Epoch 7/250\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.0252 - accuracy: 0.9926 - val_loss: 0.6457 - val_accuracy: 0.8925\n",
      "Epoch 8/250\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.6921 - val_accuracy: 0.8916\n",
      "Epoch 9/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9942\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "200000/200000 [==============================] - 25s 126us/sample - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.7153 - val_accuracy: 0.8910\n",
      "Epoch 10/250\n",
      "200000/200000 [==============================] - 26s 129us/sample - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.7166 - val_accuracy: 0.8923\n",
      "Epoch 11/250\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0179 - accuracy: 0.9949 - val_loss: 0.7223 - val_accuracy: 0.8921\n",
      "Epoch 12/250\n",
      "200000/200000 [==============================] - 25s 126us/sample - loss: 0.0174 - accuracy: 0.9950 - val_loss: 0.7381 - val_accuracy: 0.8915\n",
      "Epoch 13/250\n",
      "200000/200000 [==============================] - 25s 124us/sample - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.7435 - val_accuracy: 0.8915\n",
      "Epoch 00013: early stopping\n",
      "================================================\n",
      "ITERATION 1\n",
      "================================================\n",
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "200000/200000 [==============================] - 27s 135us/sample - loss: 0.4863 - accuracy: 0.7157 - val_loss: 0.2525 - val_accuracy: 0.9023\n",
      "Epoch 2/250\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.1419 - accuracy: 0.9516 - val_loss: 0.3249 - val_accuracy: 0.9000\n",
      "Epoch 3/250\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.0768 - accuracy: 0.9750 - val_loss: 0.4134 - val_accuracy: 0.8944\n",
      "Epoch 4/250\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.0537 - accuracy: 0.9827 - val_loss: 0.4763 - val_accuracy: 0.8941\n",
      "Epoch 5/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9870\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.0413 - accuracy: 0.9870 - val_loss: 0.5472 - val_accuracy: 0.8948\n",
      "Epoch 6/250\n",
      "200000/200000 [==============================] - 25s 124us/sample - loss: 0.0271 - accuracy: 0.9921 - val_loss: 0.6219 - val_accuracy: 0.8945\n",
      "Epoch 7/250\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.6646 - val_accuracy: 0.8933\n",
      "Epoch 8/250\n",
      "200000/200000 [==============================] - 25s 126us/sample - loss: 0.0198 - accuracy: 0.9943 - val_loss: 0.7129 - val_accuracy: 0.8927\n",
      "Epoch 9/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9949\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.7427 - val_accuracy: 0.8918\n",
      "Epoch 10/250\n",
      "200000/200000 [==============================] - 25s 126us/sample - loss: 0.0161 - accuracy: 0.9955 - val_loss: 0.7511 - val_accuracy: 0.8926\n",
      "Epoch 11/250\n",
      "200000/200000 [==============================] - 28s 142us/sample - loss: 0.0151 - accuracy: 0.9960 - val_loss: 0.7664 - val_accuracy: 0.8918\n",
      "Epoch 12/250\n",
      "200000/200000 [==============================] - 26s 129us/sample - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.7772 - val_accuracy: 0.8910\n",
      "Epoch 13/250\n",
      "200000/200000 [==============================] - 25s 124us/sample - loss: 0.0147 - accuracy: 0.9961 - val_loss: 0.7824 - val_accuracy: 0.8915\n",
      "Epoch 00013: early stopping\n",
      "================================================\n",
      "ITERATION 2\n",
      "================================================\n",
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "200000/200000 [==============================] - 26s 132us/sample - loss: 0.4869 - accuracy: 0.7193 - val_loss: 0.2594 - val_accuracy: 0.8967\n",
      "Epoch 2/250\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.1513 - accuracy: 0.9478 - val_loss: 0.2986 - val_accuracy: 0.9005\n",
      "Epoch 3/250\n",
      "200000/200000 [==============================] - 28s 141us/sample - loss: 0.0819 - accuracy: 0.9731 - val_loss: 0.4000 - val_accuracy: 0.8951\n",
      "Epoch 4/250\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0540 - accuracy: 0.9825 - val_loss: 0.4895 - val_accuracy: 0.8956\n",
      "Epoch 5/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9869\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0406 - accuracy: 0.9869 - val_loss: 0.5754 - val_accuracy: 0.8916\n",
      "Epoch 6/250\n",
      "200000/200000 [==============================] - 24s 120us/sample - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.6275 - val_accuracy: 0.8926\n",
      "Epoch 7/250\n",
      "200000/200000 [==============================] - 24s 118us/sample - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.6769 - val_accuracy: 0.8926\n",
      "Epoch 8/250\n",
      "200000/200000 [==============================] - 25s 127us/sample - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.7021 - val_accuracy: 0.8918\n",
      "Epoch 9/250\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9946\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "200000/200000 [==============================] - 24s 122us/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.7407 - val_accuracy: 0.8922\n",
      "Epoch 10/250\n",
      "200000/200000 [==============================] - 25s 125us/sample - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.7461 - val_accuracy: 0.8921\n",
      "Epoch 11/250\n",
      "200000/200000 [==============================] - 27s 134us/sample - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.7602 - val_accuracy: 0.8919\n",
      "Epoch 12/250\n",
      "200000/200000 [==============================] - 26s 131us/sample - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.7727 - val_accuracy: 0.8914\n",
      "Epoch 13/250\n",
      "200000/200000 [==============================] - 26s 128us/sample - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.7786 - val_accuracy: 0.8910\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN on the augmented dataset that was applied on the ENTIRE training dataset.\n",
    "# This step is done because the previous cell only trains up to 90% of dataset.\n",
    "\n",
    "_dataset = pd.read_csv(f'data_aug_1.0.csv')\n",
    "x_train = list(_dataset['review'])\n",
    "y_train = list(_dataset['sentiment'])\n",
    "for j in range (len(y_train)):\n",
    "    if y_train[j]=='positive':\n",
    "        y_train[j] = 1\n",
    "    else:\n",
    "        y_train[j] = 0\n",
    "y_train = np.array(y_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(train_sequences, padding=cnn_hyper_param['PADDING_TYPE'], \n",
    "                                    maxlen=cnn_hyper_param['MAX_LENGTH'], truncating=cnn_hyper_param['TRUNC_TYPE'])\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"================================================\")\n",
    "    print(f\"ITERATION {i}\")\n",
    "    print(\"================================================\")\n",
    "\n",
    "    CNN = create_cnn()\n",
    "    history = CNN.fit(training_padded, y_train, batch_size = cnn_hyper_param['BS'],\n",
    "                            epochs=250, validation_data=(testing_padded, y_test),\n",
    "                           callbacks=[reduce_lr, earlystop])\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    # save to csv:\n",
    "    hist_csv_file = f'CNN_Aug_History_{i+1}_1.0' + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Best RNN with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks for our RNN\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=8, min_lr=0.00005, verbose=1)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset and convert categorical data to integers\n",
    "\n",
    "_dataset = pd.read_csv(f'data_aug_1.0.csv')\n",
    "x_train = list(_dataset['review'])\n",
    "y_train = list(_dataset['sentiment'])\n",
    "for j in range (len(y_train)):\n",
    "    if y_train[j]=='positive':\n",
    "        y_train[j] = 1\n",
    "    else:\n",
    "        y_train[j] = 0\n",
    "y_train = np.array(y_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(train_sequences, padding=rnn_hyper_param['PADDING_TYPE'], \n",
    "                                    maxlen=rnn_hyper_param['MAX_LENGTH'], truncating=rnn_hyper_param['TRUNC_TYPE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our RNN on the various augmented datasets and store the history into a csv file.\n",
    "\n",
    "RNN_aug_hist = {}\n",
    "for size, dataset in data_dict.items():\n",
    "    print(f\"Training using {size} of dataset.\")\n",
    "    \n",
    "    # read csv for that augmented file\n",
    "    _dataset = pd.read_csv(f'data_aug_{size}.csv')\n",
    "    x_train = list(_dataset['review'])\n",
    "    y_train = list(_dataset['sentiment'])\n",
    "    for j in range (len(y_train)):\n",
    "        if y_train[j]=='positive':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = np.array(y_train)\n",
    "    train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    training_padded = pad_sequences(train_sequences, padding=RNN_hyper_param['PADDING_TYPE'], \n",
    "                                        maxlen=RNN_hyper_param['MAX_LENGTH'], truncating=RNN_hyper_param['TRUNC_TYPE'])\n",
    "\n",
    "    RNN = create_rnn()\n",
    "    history = RNN.fit(training_padded, y_train, batch_size = RNN_hyper_param['BS'],\n",
    "                            epochs=250, validation_data=(testing_padded, y_test),\n",
    "                           callbacks=[reduce_lr, earlystop])\n",
    "\n",
    "    RNN_aug_hist[f'{size}'] = history\n",
    "    \n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    # save to csv:\n",
    "    hist_csv_file = 'CNN_Aug_History_' + str(size) + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
