{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from random import sample\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hyper_param = {\n",
    "    'VOCAB_SIZE':50000,\n",
    "    'EPOCHS':150,\n",
    "    'BS':1024,\n",
    "    'LR':0.005,\n",
    "    'OOV_TOK':\"<OOV>\",\n",
    "    'MAX_LENGTH':520,\n",
    "    'PADDING_TYPE':\"post\",\n",
    "    'TRUNC_TYPE':\"post\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hyper_param = {\n",
    "    'VOCAB_SIZE':50000,\n",
    "    'EPOCHS':150,\n",
    "    'BS':1024,\n",
    "    'LR':0.001,\n",
    "    'OOV_TOK':\"<OOV>\",\n",
    "    'MAX_LENGTH':520,\n",
    "    'PADDING_TYPE':\"post\",\n",
    "    'TRUNC_TYPE':\"post\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep to the same size of test set we've been using to test how well it generalizes.\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], \n",
    "                                                    test_size=0.2, random_state=0, \n",
    "                                                    stratify=data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38414</th>\n",
       "      <td>The notion of marital fidelity portrayed in th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24010</th>\n",
       "      <td>What a good film! Made Men is a great action m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29873</th>\n",
       "      <td>Joe Don Baker. He was great in \"Walking Tall\" ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Monarch Cove was one of the best Friday night'...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15107</th>\n",
       "      <td>This film is so unbelievable; - the whole prem...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "38414  The notion of marital fidelity portrayed in th...  positive\n",
       "24010  What a good film! Made Men is a great action m...  positive\n",
       "29873  Joe Don Baker. He was great in \"Walking Tall\" ...  negative\n",
       "2868   Monarch Cove was one of the best Friday night'...  positive\n",
       "15107  This film is so unbelievable; - the whole prem...  negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {\n",
    "    'review':x_train,\n",
    "    'sentiment':y_train\n",
    "}\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main method used to perform data augmentation. It calls on all methods below it\n",
    "# to perform data augmentation on eaech sentence.\n",
    "\n",
    "def data_augment(df, alpha):\n",
    "    df_arr = []\n",
    "    for index, row in df.iterrows():\n",
    "        _arr = []\n",
    "        # Remove <br /> and split data into sentence length\n",
    "        review = re.split(\"\\.+\", row['review'].replace('<br />', ''))\n",
    "        \n",
    "        # Perform data augmentation on each sentence.\n",
    "        for idx, sentence in enumerate(review):\n",
    "            sentence = random_swap(sentence, alpha)\n",
    "            sentence = random_insertion(sentence, alpha)\n",
    "            sentence = synonym_replacement(sentence,alpha)\n",
    "            sentence = random_deletion(sentence, alpha)\n",
    "\n",
    "            review[idx] = sentence\n",
    "\n",
    "        # rejoin the split sentences into a single paragraph again\n",
    "        separator = '. '\n",
    "        review = separator.join(review)\n",
    "        _arr.append(review)\n",
    "        _arr.append(row['sentiment'])\n",
    "        df_arr.append(_arr)\n",
    "    \n",
    "    return pd.DataFrame(np.array(df_arr), columns=['review','sentiment']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly swap two words in a sentence. This is done alpha*length of sentence times\n",
    "\n",
    "def random_swap(sentence, alpha):\n",
    "    sentence = sentence.split()\n",
    "    for i in range(int(alpha*len(sentence))):\n",
    "        sample = random.sample(range(0, len(sentence)-1), 2)\n",
    "        word_1 = sentence[sample[0]]\n",
    "        word_2 = sentence[sample[1]]\n",
    "        sentence[sample[0]] = word_2\n",
    "        sentence[sample[1]] = word_1\n",
    "        \n",
    "    return ' '.join(word for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly deletes words in a sentence iwth probability of alpha*length of sentence\n",
    "\n",
    "def random_deletion(sentence, alpha):\n",
    "    sentence = sentence.split()\n",
    "    sample = (random.sample(range(0, len(sentence)-1), int(alpha*len(sentence))))\n",
    "    sample.sort(reverse=True)\n",
    "\n",
    "    for idx in sample:\n",
    "        del sentence[idx]\n",
    "        \n",
    "    return ' '.join(word for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick non-stopwords in a sentence with probability alpha*length of sentence\n",
    "# Replace these words with a random synonym of itself.\n",
    "\n",
    "def synonym_replacement(sentence, alpha):\n",
    "    sentence = sentence.split()\n",
    "    # Remove punctuations except for inverted commas and hyphens as these can have meanings\n",
    "    sentence = [''.join(c for c in s if c not in [',!#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~']) for s in sentence]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    non_stopwords_idx = []    \n",
    "    \n",
    "    # Find indices of words that are not stop words\n",
    "    for i in range (len(sentence)):\n",
    "        if sentence[i].lower() not in stop_words:\n",
    "            non_stopwords_idx.append(i)\n",
    "    \n",
    "    # If we need to replace more words than there are non-stop words, then just replace all non-stop words\n",
    "    if int(alpha*len(sentence)) > len(non_stopwords_idx):\n",
    "        words = sample(non_stopwords_idx, len(non_stopwords_idx))\n",
    "    else:\n",
    "        words = sample(non_stopwords_idx, int(alpha*len(sentence)))\n",
    "\n",
    "    # Get synonym for each word to be replaced\n",
    "    for word in words:\n",
    "        synonym_list = get_synonym(sentence[word])\n",
    "        if not synonym_list:\n",
    "            continue   \n",
    "        # Replace word with a randomy synonym from a list of synonyms\n",
    "        synonym = sample(synonym_list, 1)\n",
    "        sentence[word] = synonym[0]\n",
    "    \n",
    "    return ' '.join(word for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly inserts a synonym of a non-stop word into a sentence. Probability is given by alpha*sentence length\n",
    "\n",
    "def random_insertion(sentence, alpha):\n",
    "    sentence = sentence.split()\n",
    "    \n",
    "    # Remove punctuations except for inverted commas and hyphens as these can have meanings\n",
    "    sentence = [''.join(c for c in s if c not in [',!#$%&\\()*+,./:;<=>?@[\\\\]^_`{|}~']) for s in sentence]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    non_stopwords_idx = []    \n",
    "    \n",
    "    # Find indices of words that are not stop words\n",
    "    for i in range (len(sentence)):\n",
    "        if sentence[i].lower() not in stop_words:\n",
    "            non_stopwords_idx.append(i)\n",
    "    \n",
    "    # If we need to replace more words than there are non-stop words, then just replace all non-stop words\n",
    "    if int(alpha*len(sentence)) > len(non_stopwords_idx):\n",
    "        words = sample(non_stopwords_idx, len(non_stopwords_idx))\n",
    "    else:\n",
    "        words = sample(non_stopwords_idx, int(alpha*len(sentence)))\n",
    "\n",
    "    # Get synonym for each word to be replaced\n",
    "    for word in words:\n",
    "        synonym_list = get_synonym(sentence[word])\n",
    "        # If no synonyms is found, continue\n",
    "        if not synonym_list:\n",
    "            continue\n",
    "        synonym = sample(synonym_list, 1)\n",
    "        sentence.insert(random.randint(0, len(sentence)-1), synonym[0])\n",
    "        \n",
    "    return ' '.join(word for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a random synonym given a word\n",
    "\n",
    "def get_synonym(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonyms.add(synonym)\n",
    "\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [39:09<00:00, 587.46s/it]\n"
     ]
    }
   ],
   "source": [
    "#  Perform data augmentation on full training dataset.\n",
    "aug_df = []\n",
    "aug_df_list = []\n",
    "for i in tqdm(range(4)):\n",
    "    aug_df = data_augment(df, 0.1)\n",
    "    aug_df_list.append(aug_df)\n",
    "\n",
    "for dataframes in aug_df_list:\n",
    "    df = pd.concat([df, dataframes], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the augmented datset into a csv file. The data augmentation code for all other data subsets is not in\n",
    "# this notebook as it was on the GPU cluster but was deleted away. They all follow the same logic\n",
    "\n",
    "hist_csv_file = 'data_aug_1.0.csv'\n",
    "with open(hist_csv_file, mode='w', encoding='UTF-8') as f:\n",
    "    df.to_csv(f, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation on all  other subsets of dataset\n",
    "\n",
    "for size, dataset in data_dict.items():\n",
    "    print(f\"Data Augmenting on dataset of size: {size}\")\n",
    "    print(\"Current size of dataset:\", len(dataset))\n",
    "    _history = []\n",
    "    _dataset_list = []\n",
    "    if len(dataset) <= 500:\n",
    "        alpha = 0.05\n",
    "        iterate = 16\n",
    "    elif len(dataset) <= 2000:\n",
    "        alpha = 0.05\n",
    "        iterate = 8\n",
    "    else:\n",
    "        alpha = 0.1\n",
    "        iterate = 4\n",
    "        \n",
    "    for i in tqdm(range(iterate)):\n",
    "        _dataset = data_augment(dataset, alpha)\n",
    "        _dataset_list.append(_dataset)\n",
    "    for item in _dataset_list:\n",
    "        dataset = pd.concat([dataset, item], ignore_index = True)\n",
    "    \n",
    "    data_dict[size] = dataset\n",
    "    print(\"Augmented Size of Dataset:\", len(data_dict[size]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write augmented data into csv\n",
    "\n",
    "for key, items in data_dict.items():\n",
    "    items.to_csv(f'data_aug_{key}.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
